/**\n * Cache Integration Tests\n * Comprehensive testing of the intelligent search caching layer\n */\n\nimport { describe, beforeEach, afterEach, it, expect, jest } from '@jest/globals';\nimport { CachedSearchService } from '../../../src/services/search/CachedSearchService';\nimport { CacheService } from '../../../src/services/CacheService';\nimport { CacheWarmer } from '../../../src/services/cache/CacheWarmer';\nimport { CacheMiddleware } from '../../../src/middleware/cacheMiddleware';\nimport { BatchDocumentRetriever } from '../../../src/services/search/BatchDocumentRetriever';\nimport { CacheMetricsCollector } from '../../../src/monitoring/CacheMetrics';\nimport { CacheMaintenanceJobs } from '../../../src/jobs/cacheMaintenance';\nimport { KBEntry } from '../../../src/types';\n\n// Mock data\nconst createMockKBEntries = (count: number): KBEntry[] => {\n  const entries: KBEntry[] = [];\n  const categories = ['System', 'Performance', 'Error', 'Configuration', 'Other'];\n  const tags = ['vsam', 'jcl', 'batch', 'online', 'database', 'network', 'security'];\n  \n  for (let i = 1; i <= count; i++) {\n    entries.push({\n      id: `entry-${i}`,\n      title: `Test Entry ${i}`,\n      problem: `Problem description for entry ${i}`,\n      solution: `Solution for entry ${i}`,\n      category: categories[i % categories.length],\n      tags: [tags[i % tags.length], tags[(i + 1) % tags.length]],\n      created_at: new Date(Date.now() - (i * 24 * 60 * 60 * 1000)),\n      updated_at: new Date(Date.now() - (i * 12 * 60 * 60 * 1000)),\n      usage_count: Math.floor(Math.random() * 100),\n      success_count: Math.floor(Math.random() * 80),\n      failure_count: Math.floor(Math.random() * 20)\n    });\n  }\n  \n  return entries;\n};\n\ndescribe('Cache Integration Tests', () => {\n  let cachedSearchService: CachedSearchService;\n  let cacheService: CacheService;\n  let cacheWarmer: CacheWarmer;\n  let cacheMiddleware: CacheMiddleware;\n  let batchRetriever: BatchDocumentRetriever;\n  let metricsCollector: CacheMetricsCollector;\n  let maintenanceJobs: CacheMaintenanceJobs;\n  let mockEntries: KBEntry[];\n\n  beforeEach(async () => {\n    // Create mock entries\n    mockEntries = createMockKBEntries(100);\n    \n    // Initialize cache service\n    cacheService = new CacheService({\n      maxSize: 1000,\n      defaultTTL: 300000, // 5 minutes\n      checkPeriod: 60000\n    });\n    \n    // Initialize cached search service\n    cachedSearchService = new CachedSearchService({\n      cache: {\n        enabled: true,\n        layers: {\n          l1: { size: 100, ttl: 60000 },\n          l2: { size: 500, ttl: 300000 },\n          l3: { size: 1000, ttl: 900000 }\n        },\n        warming: {\n          enabled: true,\n          strategies: ['popular', 'recent'],\n          schedule: '*/5 * * * *'\n        },\n        invalidation: {\n          enabled: true,\n          smartCascade: true,\n          maxBatchSize: 100\n        },\n        monitoring: {\n          enabled: true,\n          metricsInterval: 30000,\n          alertThresholds: {\n            hitRate: 0.7,\n            responseTime: 1000,\n            errorRate: 0.05\n          }\n        }\n      }\n    });\n    \n    // Initialize cache warmer\n    cacheWarmer = new CacheWarmer(cachedSearchService, cacheService);\n    \n    // Initialize cache middleware\n    cacheMiddleware = new CacheMiddleware(cacheService, {\n      enabled: true,\n      defaultTTL: 300000,\n      compression: { enabled: true, threshold: 1024, algorithm: 'gzip' }\n    });\n    \n    // Initialize batch retriever\n    batchRetriever = new BatchDocumentRetriever(cacheService, {\n      batchSize: 50,\n      cacheEnabled: true,\n      prefetchEnabled: true\n    });\n    \n    // Initialize metrics collector\n    metricsCollector = new CacheMetricsCollector(\n      cachedSearchService,\n      cacheService,\n      cacheMiddleware,\n      batchRetriever,\n      {\n        enabled: true,\n        collectionInterval: 1000, // 1 second for testing\n        retentionPeriod: 1\n      }\n    );\n    \n    // Initialize maintenance jobs\n    maintenanceJobs = new CacheMaintenanceJobs(\n      cachedSearchService,\n      cacheService,\n      cacheWarmer,\n      cacheMiddleware,\n      {\n        enabled: true,\n        schedule: {\n          cleanup: '0 */6 * * *',\n          optimization: '0 */12 * * *',\n          warming: '*/15 * * *',\n          healthCheck: '*/5 * * *',\n          metrics: '* * * * *'\n        }\n      }\n    );\n    \n    // Initialize all services\n    await cachedSearchService.initialize(mockEntries);\n    await batchRetriever.initialize(mockEntries);\n  });\n\n  afterEach(async () => {\n    await cachedSearchService.shutdown();\n    await cacheService.close();\n    metricsCollector.stop();\n    await maintenanceJobs.stop();\n  });\n\n  describe('Basic Cache Functionality', () => {\n    it('should cache search results effectively', async () => {\n      const query = 'test query';\n      const options = { limit: 10 };\n      \n      // First search - should be cache miss\n      const result1 = await cachedSearchService.search(query, options);\n      expect(result1).toBeDefined();\n      expect(result1.metrics.cacheHit).toBe(false);\n      \n      // Second search - should be cache hit\n      const result2 = await cachedSearchService.search(query, options);\n      expect(result2).toBeDefined();\n      expect(result2.metrics.cacheHit).toBe(true);\n      \n      // Results should be identical\n      expect(result2.results).toEqual(result1.results);\n    });\n\n    it('should handle cache invalidation correctly', async () => {\n      const query = 'invalidation test';\n      \n      // Cache a search result\n      await cachedSearchService.search(query, { limit: 5 });\n      \n      // Invalidate cache\n      const invalidated = await cachedSearchService.invalidateCache('*test*');\n      expect(invalidated.invalidated).toBeGreaterThan(0);\n      \n      // Next search should be cache miss\n      const result = await cachedSearchService.search(query, { limit: 5 });\n      expect(result.metrics.cacheHit).toBe(false);\n    });\n\n    it('should respect TTL settings', async () => {\n      const query = 'ttl test';\n      \n      // Cache with short TTL\n      await cacheService.set('test-key', 'test-value', 100); // 100ms TTL\n      \n      // Should be available immediately\n      const value1 = await cacheService.get('test-key');\n      expect(value1).toBe('test-value');\n      \n      // Wait for TTL to expire\n      await new Promise(resolve => setTimeout(resolve, 150));\n      \n      // Should be expired\n      const value2 = await cacheService.get('test-key');\n      expect(value2).toBeNull();\n    });\n  });\n\n  describe('Cache Warming', () => {\n    it('should warm cache with popular queries', async () => {\n      // Simulate some popular queries\n      const popularQueries = ['error handling', 'performance tuning', 'batch processing'];\n      \n      for (const query of popularQueries) {\n        await cachedSearchService.search(query, { limit: 10 });\n        await cachedSearchService.search(query, { limit: 10 }); // Hit it twice to make it popular\n      }\n      \n      // Clear cache\n      await cachedSearchService.invalidateCache('*');\n      \n      // Warm cache\n      const warmResult = await cacheWarmer.warmCache('popular-queries');\n      expect(warmResult.warmed).toBeGreaterThan(0);\n      \n      // Verify that popular queries are now cached\n      for (const query of popularQueries) {\n        const result = await cachedSearchService.search(query, { limit: 10 });\n        expect(result.metrics.cacheHit).toBe(true);\n      }\n    });\n\n    it('should perform adaptive warming based on performance', async () => {\n      const performanceMetrics = {\n        hitRate: 0.5, // Low hit rate\n        avgResponseTime: 1200, // High response time\n        throughput: 50,\n        errorRate: 0.02\n      };\n      \n      const results = await cacheWarmer.adaptiveWarming(performanceMetrics);\n      expect(results).toBeDefined();\n      expect(results.length).toBeGreaterThan(0);\n      \n      const totalWarmed = results.reduce((sum, r) => sum + r.warmed, 0);\n      expect(totalWarmed).toBeGreaterThan(0);\n    });\n\n    it('should warm cache for specific users', async () => {\n      const userContext = {\n        userId: 'test-user-123',\n        role: 'developer',\n        preferences: { categories: ['System', 'Performance'] },\n        recentQueries: ['memory error', 'cpu usage', 'disk space'],\n        sessionData: {}\n      };\n      \n      const result = await cacheWarmer.warmForUser(userContext);\n      expect(result.warmed).toBeGreaterThan(0);\n      expect(result.strategy).toBe('user-specific');\n    });\n  });\n\n  describe('Batch Document Retrieval', () => {\n    it('should retrieve documents in batches efficiently', async () => {\n      const documentIds = mockEntries.slice(0, 20).map(e => e.id);\n      \n      const startTime = Date.now();\n      const documents = await batchRetriever.getDocuments(documentIds);\n      const batchTime = Date.now() - startTime;\n      \n      expect(documents.size).toBe(documentIds.length);\n      \n      // Individual retrieval for comparison\n      const individualStartTime = Date.now();\n      for (const id of documentIds.slice(0, 5)) { // Test with smaller subset\n        await batchRetriever.getDocument(id);\n      }\n      const individualTime = Date.now() - individualStartTime;\n      \n      // Batch should be more efficient\n      expect(batchTime).toBeLessThan(individualTime * 2); // Allow some margin\n    });\n\n    it('should handle cache hits and misses in batch operations', async () => {\n      const documentIds = mockEntries.slice(0, 10).map(e => e.id);\n      \n      // Pre-cache some documents\n      const preCachedIds = documentIds.slice(0, 5);\n      await batchRetriever.getDocuments(preCachedIds);\n      \n      // Clear cache service metrics\n      const initialMetrics = batchRetriever.getMetrics();\n      \n      // Retrieve all documents (some cached, some not)\n      await batchRetriever.getDocuments(documentIds);\n      \n      const finalMetrics = batchRetriever.getMetrics();\n      expect(finalMetrics.cacheHitRate).toBeGreaterThan(initialMetrics.cacheHitRate);\n    });\n\n    it('should prefetch related documents', async () => {\n      const baseDocuments = mockEntries.slice(0, 5);\n      const baseIds = baseDocuments.map(d => d.id);\n      \n      const prefetched = await batchRetriever.prefetchDocuments(baseIds, 'related');\n      expect(prefetched).toBeGreaterThanOrEqual(0);\n      \n      // Verify prefetching improved performance\n      const metrics = batchRetriever.getMetrics();\n      expect(metrics).toBeDefined();\n    });\n  });\n\n  describe('HTTP Cache Middleware', () => {\n    it('should create proper cache middleware', () => {\n      const middleware = cacheMiddleware.middleware();\n      expect(middleware).toBeDefined();\n      expect(typeof middleware).toBe('function');\n    });\n\n    it('should create invalidation middleware', () => {\n      const invalidationMiddleware = cacheMiddleware.invalidationMiddleware();\n      expect(invalidationMiddleware).toBeDefined();\n      expect(typeof invalidationMiddleware).toBe('function');\n    });\n\n    it('should track HTTP cache metrics', async () => {\n      const metrics = cacheMiddleware.getMetrics();\n      expect(metrics).toBeDefined();\n      expect(metrics.requests).toBeDefined();\n      expect(metrics.performance).toBeDefined();\n      expect(metrics.storage).toBeDefined();\n    });\n  });\n\n  describe('Metrics Collection', () => {\n    it('should collect metrics from all cache components', async () => {\n      metricsCollector.start();\n      \n      // Perform some operations to generate metrics\n      await cachedSearchService.search('metrics test', { limit: 5 });\n      await batchRetriever.getDocument('entry-1');\n      \n      // Wait for metrics collection\n      await new Promise(resolve => setTimeout(resolve, 1100));\n      \n      const summary = metricsCollector.getSummary();\n      expect(summary).toBeDefined();\n      expect(summary.overall.status).toMatch(/healthy|degraded|critical/);\n      expect(summary.performance).toBeDefined();\n      expect(summary.trends).toBeDefined();\n      \n      metricsCollector.stop();\n    });\n\n    it('should generate alerts for threshold violations', async () => {\n      metricsCollector.start();\n      \n      const alertPromise = new Promise(resolve => {\n        metricsCollector.once('alert', resolve);\n      });\n      \n      // Force metric collection to generate potential alerts\n      await metricsCollector.forceCollection();\n      \n      // Check if any alerts were generated\n      const activeAlerts = metricsCollector.getActiveAlerts();\n      expect(Array.isArray(activeAlerts)).toBe(true);\n      \n      metricsCollector.stop();\n    });\n\n    it('should export metrics in different formats', async () => {\n      metricsCollector.start();\n      \n      // Collect some data\n      await cachedSearchService.search('export test', { limit: 3 });\n      await new Promise(resolve => setTimeout(resolve, 1100));\n      \n      // Test Prometheus export\n      const prometheusMetrics = metricsCollector.getPrometheusMetrics();\n      expect(typeof prometheusMetrics).toBe('string');\n      expect(prometheusMetrics).toContain('cache_');\n      \n      metricsCollector.stop();\n    });\n  });\n\n  describe('Cache Maintenance Jobs', () => {\n    it('should run cleanup job successfully', async () => {\n      // Add some data to cache first\n      await cachedSearchService.search('cleanup test', { limit: 5 });\n      await cacheService.set('temp-key', 'temp-value', 100); // Short TTL\n      \n      // Wait for TTL to expire\n      await new Promise(resolve => setTimeout(resolve, 150));\n      \n      // Run cleanup job\n      const result = await maintenanceJobs.runJob('cleanup');\n      expect(result).toBeDefined();\n      expect(result.status).toMatch(/success|partial/);\n      expect(result.details.processed).toBeGreaterThan(0);\n    });\n\n    it('should run optimization job successfully', async () => {\n      // Perform some searches to generate data for optimization\n      const queries = ['opt1', 'opt2', 'opt3', 'opt4', 'opt5'];\n      \n      for (const query of queries) {\n        await cachedSearchService.search(query, { limit: 5 });\n      }\n      \n      const result = await maintenanceJobs.runJob('optimization');\n      expect(result).toBeDefined();\n      expect(result.status).toMatch(/success|partial/);\n    });\n\n    it('should run warming job successfully', async () => {\n      const result = await maintenanceJobs.runJob('warming');\n      expect(result).toBeDefined();\n      expect(result.status).toMatch(/success|partial/);\n    });\n\n    it('should run health check job successfully', async () => {\n      const result = await maintenanceJobs.runJob('healthCheck');\n      expect(result).toBeDefined();\n      expect(result.status).toMatch(/success|partial/);\n    });\n  });\n\n  describe('Integration Scenarios', () => {\n    it('should handle high-load search scenario', async () => {\n      const searchPromises: Promise<any>[] = [];\n      const queries = [\n        'performance', 'error', 'system', 'batch', 'online',\n        'database', 'network', 'security', 'configuration', 'troubleshooting'\n      ];\n      \n      // Simulate concurrent searches\n      for (let i = 0; i < 50; i++) {\n        const query = queries[i % queries.length];\n        searchPromises.push(\n          cachedSearchService.search(query, { limit: 10 })\n        );\n      }\n      \n      const results = await Promise.all(searchPromises);\n      expect(results.length).toBe(50);\n      \n      // Verify that caching improved performance\n      const metrics = cachedSearchService.getMetrics();\n      expect(metrics.hitRates.overall).toBeGreaterThan(0);\n    });\n\n    it('should maintain cache coherence during document updates', async () => {\n      const query = 'coherence test';\n      \n      // Initial search\n      const result1 = await cachedSearchService.search(query, { limit: 5 });\n      \n      // Add a new document\n      const newEntry: KBEntry = {\n        id: 'new-entry-coherence',\n        title: 'Coherence Test Entry',\n        problem: 'Testing cache coherence',\n        solution: 'Solution for coherence test',\n        category: 'System',\n        tags: ['test', 'coherence'],\n        created_at: new Date(),\n        updated_at: new Date(),\n        usage_count: 0,\n        success_count: 0,\n        failure_count: 0\n      };\n      \n      await cachedSearchService.addDocument(newEntry);\n      \n      // Search again - should reflect the new document\n      const result2 = await cachedSearchService.search(query, { limit: 5 });\n      \n      // Cache should have been invalidated and refreshed\n      expect(result2.metrics.cacheHit).toBe(false); // First search after invalidation\n    });\n\n    it('should handle cache failures gracefully', async () => {\n      // Simulate cache failure by closing cache service\n      await cacheService.close();\n      \n      // Search should still work (fallback to direct search)\n      const result = await cachedSearchService.search('fallback test', { limit: 5 });\n      expect(result).toBeDefined();\n      expect(result.results).toBeDefined();\n    });\n\n    it('should demonstrate end-to-end performance improvements', async () => {\n      const query = 'performance improvement test';\n      const iterations = 10;\n      \n      // First run - populate cache\n      await cachedSearchService.search(query, { limit: 10 });\n      \n      // Measure cached performance\n      const cachedStartTime = Date.now();\n      \n      for (let i = 0; i < iterations; i++) {\n        await cachedSearchService.search(query, { limit: 10 });\n      }\n      \n      const cachedTime = Date.now() - cachedStartTime;\n      \n      // Clear cache and measure uncached performance\n      await cachedSearchService.invalidateCache('*');\n      \n      const uncachedStartTime = Date.now();\n      \n      for (let i = 0; i < iterations; i++) {\n        await cachedSearchService.search(query, { limit: 10 });\n      }\n      \n      const uncachedTime = Date.now() - uncachedStartTime;\n      \n      // Cached should be significantly faster\n      console.log(`Cached time: ${cachedTime}ms, Uncached time: ${uncachedTime}ms`);\n      expect(cachedTime).toBeLessThan(uncachedTime * 0.8); // At least 20% improvement\n    });\n  });\n\n  describe('Error Handling', () => {\n    it('should handle search errors gracefully', async () => {\n      // This would depend on your specific error scenarios\n      // For example, invalid queries or service failures\n      \n      try {\n        await cachedSearchService.search('', { limit: -1 }); // Invalid params\n      } catch (error) {\n        expect(error).toBeDefined();\n      }\n      \n      // Service should still be functional\n      const result = await cachedSearchService.search('valid query', { limit: 5 });\n      expect(result).toBeDefined();\n    });\n\n    it('should handle cache service errors', async () => {\n      // Simulate cache error by using invalid cache key\n      const metrics = cacheService.stats();\n      expect(metrics).toBeDefined();\n      \n      // Cache errors shouldn't break the search functionality\n      const result = await cachedSearchService.search('error handling test', { limit: 5 });\n      expect(result).toBeDefined();\n    });\n  });\n\n  describe('Performance Validation', () => {\n    it('should meet performance SLAs', async () => {\n      const queries = ['sla1', 'sla2', 'sla3', 'sla4', 'sla5'];\n      const responseTimes: number[] = [];\n      \n      // Warm up cache\n      for (const query of queries) {\n        await cachedSearchService.search(query, { limit: 10 });\n      }\n      \n      // Measure cached performance\n      for (const query of queries) {\n        const startTime = Date.now();\n        await cachedSearchService.search(query, { limit: 10 });\n        responseTimes.push(Date.now() - startTime);\n      }\n      \n      const avgResponseTime = responseTimes.reduce((sum, time) => sum + time, 0) / responseTimes.length;\n      const maxResponseTime = Math.max(...responseTimes);\n      \n      // Performance SLAs\n      expect(avgResponseTime).toBeLessThan(100); // Average under 100ms\n      expect(maxResponseTime).toBeLessThan(500); // Maximum under 500ms\n      \n      console.log(`Average response time: ${avgResponseTime}ms, Max: ${maxResponseTime}ms`);\n    });\n\n    it('should maintain hit rate above threshold', async () => {\n      const queries = ['hit1', 'hit2', 'hit3', 'hit4', 'hit5'];\n      \n      // Perform initial searches\n      for (const query of queries) {\n        await cachedSearchService.search(query, { limit: 10 });\n      }\n      \n      // Perform repeat searches\n      for (const query of queries) {\n        await cachedSearchService.search(query, { limit: 10 });\n      }\n      \n      const metrics = cachedSearchService.getMetrics();\n      expect(metrics.hitRates.overall).toBeGreaterThan(0.5); // At least 50% hit rate\n    });\n  });\n});